#度量的学习:
#1.欧氏距离,2曼哈顿距离3.切比雪夫距离4.闵科夫斯基距离5.标准化欧氏距离6.马氏距离
#7.夹角余弦距离用于度量两个角度之见的差距.8.pearson相关系数
#9. 汉明距离(Hamming distance):
#    两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作
#    的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。
##10.杰卡德相似系数(Jaccard similarity coefficient)
##用于度量两个set之见距离
##
##11.布雷柯蒂斯距离(Bray Curtis Distance)
##
##Bray Curtis距离主要用于生态学和环境科学，计算坐标之间的距离。该
##距离取值在[0,1]之间。它也可以用来计算样本之间的差异。

#概率之见的度量
## 所用的其实就是统计量
## 1.卡方检验经常用来检验某一种观测分布是不是符合某一类典型的理论分布（如二项分布，正态分
##   布等)。观察频数与期望频数越接近，两者之间的差异越小，χ2值越小；如果两个分
##   布完全一致，χ2值为0；反之，观察频数与期望频数差别越大，两者之间的差异越大，χ2值
##   越大.
## 2.交叉熵
##     两个独立符号所产生的不确定性应等于各自不确定性之和，即f（P1，P2）=f（P1）+f（P2），这称为可加性。
##     所以显然是log函数,不确定性也就是表示信息量,一个确定事件信息量是0,而两个独立时间同时发生那么
##     他们的信息量显然要可加性.所以是log
## 3.相对熵(relative entropy)又称为KL散度
##      相对熵是交叉熵与信息熵的差值。
## 4.js散度（Jensen-Shannon）
##
##      因为kl散度不具对称性，因此js散度在kl散度的基础上进行了改进：
